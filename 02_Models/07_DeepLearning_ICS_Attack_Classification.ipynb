{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85e73f5-fb97-47ca-a991-42d87582ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "     --------------------------------------- 11.3/11.3 MB 19.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\tf310env\\lib\\site-packages (1.23.5)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "     ------------------------------------- 347.8/347.8 kB 22.5 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\tf310env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\tf310env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1ff1b2-6794-41fe-b430-2a3b2d60d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pandas version: 2.3.1\n",
      "âœ… Numpy version: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… Pandas version:\", pd.__version__)\n",
    "print(\"âœ… Numpy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055fe8cc-79b3-4f08-97e5-8b55ff5fad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "     --------------------------------------- 10.7/10.7 MB 15.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "     ------------------------------------- 307.7/307.7 kB 18.6 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\administrator\\tf310env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\administrator\\tf310env\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b5d368-af3c-49a5-b452-a99e714fb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7441747b-ca61-40b5-b69a-3a30518d39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data prepared successfully.\n",
      "ðŸ“ X_train_scaled shape: (99245, 63)\n",
      "ðŸ“ y_train_cat shape: (99245, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# === 1. Load the preprocessed dataset ===\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\preprocessed_dataset_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === 2. Separate features and target ===\n",
    "X = df.drop('Attack_enc', axis=1)\n",
    "y = df['Attack_enc']\n",
    "\n",
    "# === 3. Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# === 4. Feature scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === 5. One-hot encode the target for multi-class classification ===\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "print(\"âœ… Data prepared successfully.\")\n",
    "print(\"ðŸ“ X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"ðŸ“ y_train_cat shape:\", y_train_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ac3375-30c1-4b89-bfa5-609a407a64af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "621/621 [==============================] - 7s 7ms/step - loss: 0.9946 - accuracy: 0.7301 - val_loss: 0.8651 - val_accuracy: 0.7558\n",
      "Epoch 2/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.8565 - accuracy: 0.7570 - val_loss: 0.8344 - val_accuracy: 0.7660\n",
      "Epoch 3/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.8294 - accuracy: 0.7660 - val_loss: 0.8245 - val_accuracy: 0.7648\n",
      "Epoch 4/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.8181 - accuracy: 0.7684 - val_loss: 0.8168 - val_accuracy: 0.7713\n",
      "Epoch 5/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.8113 - accuracy: 0.7702 - val_loss: 0.8097 - val_accuracy: 0.7683\n",
      "Epoch 6/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.8068 - accuracy: 0.7710 - val_loss: 0.8084 - val_accuracy: 0.7733\n",
      "Epoch 7/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.8056 - accuracy: 0.7727 - val_loss: 0.8079 - val_accuracy: 0.7705\n",
      "Epoch 8/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7999 - accuracy: 0.7727 - val_loss: 0.8058 - val_accuracy: 0.7695\n",
      "Epoch 9/30\n",
      "621/621 [==============================] - 3s 6ms/step - loss: 0.7993 - accuracy: 0.7735 - val_loss: 0.8039 - val_accuracy: 0.7746\n",
      "Epoch 10/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7965 - accuracy: 0.7736 - val_loss: 0.8034 - val_accuracy: 0.7717\n",
      "Epoch 11/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7958 - accuracy: 0.7743 - val_loss: 0.8028 - val_accuracy: 0.7739\n",
      "Epoch 12/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7934 - accuracy: 0.7744 - val_loss: 0.8021 - val_accuracy: 0.7728\n",
      "Epoch 13/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7929 - accuracy: 0.7747 - val_loss: 0.8034 - val_accuracy: 0.7728\n",
      "Epoch 14/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7919 - accuracy: 0.7756 - val_loss: 0.8072 - val_accuracy: 0.7679\n",
      "Epoch 15/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7920 - accuracy: 0.7752 - val_loss: 0.8050 - val_accuracy: 0.7718\n",
      "Epoch 16/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7900 - accuracy: 0.7756 - val_loss: 0.8025 - val_accuracy: 0.7727\n",
      "Epoch 17/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7911 - accuracy: 0.7756 - val_loss: 0.8009 - val_accuracy: 0.7724\n",
      "Epoch 18/30\n",
      "621/621 [==============================] - 5s 7ms/step - loss: 0.7900 - accuracy: 0.7754 - val_loss: 0.7984 - val_accuracy: 0.7742\n",
      "Epoch 19/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7889 - accuracy: 0.7762 - val_loss: 0.8009 - val_accuracy: 0.7721\n",
      "Epoch 20/30\n",
      "621/621 [==============================] - 3s 6ms/step - loss: 0.7882 - accuracy: 0.7759 - val_loss: 0.8028 - val_accuracy: 0.7724\n",
      "Epoch 21/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7869 - accuracy: 0.7758 - val_loss: 0.7983 - val_accuracy: 0.7742\n",
      "Epoch 22/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7863 - accuracy: 0.7764 - val_loss: 0.7982 - val_accuracy: 0.7736\n",
      "Epoch 23/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7849 - accuracy: 0.7765 - val_loss: 0.7996 - val_accuracy: 0.7724\n",
      "Epoch 24/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7848 - accuracy: 0.7764 - val_loss: 0.7992 - val_accuracy: 0.7743\n",
      "Epoch 25/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7857 - accuracy: 0.7767 - val_loss: 0.7967 - val_accuracy: 0.7742\n",
      "Epoch 26/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7842 - accuracy: 0.7772 - val_loss: 0.7974 - val_accuracy: 0.7720\n",
      "Epoch 27/30\n",
      "621/621 [==============================] - 3s 6ms/step - loss: 0.7837 - accuracy: 0.7778 - val_loss: 0.7957 - val_accuracy: 0.7744\n",
      "Epoch 28/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7828 - accuracy: 0.7775 - val_loss: 0.7962 - val_accuracy: 0.7737\n",
      "Epoch 29/30\n",
      "621/621 [==============================] - 5s 7ms/step - loss: 0.7828 - accuracy: 0.7777 - val_loss: 0.7957 - val_accuracy: 0.7742\n",
      "Epoch 30/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7820 - accuracy: 0.7773 - val_loss: 0.7953 - val_accuracy: 0.7726\n",
      "âœ… Training complete.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# === 1. Build the model ===\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')  # 18 classes\n",
    "])\n",
    "\n",
    "# === 2. Compile the model ===\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === 3. Define EarlyStopping to avoid overfitting ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# === 4. Train the model ===\n",
    "history = model.fit(X_train_scaled, y_train_cat,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=30,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n",
    "print(\"âœ… Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eac88da-32f9-4b3c-b9df-4d1ca03522ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Administrator\\\\tf310env\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached numpy-1.23.5-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.23.5 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f972e2-d197-4ae6-8b15-f4518427796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "1.23.5\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aef27dd-a915-4a93-a161-5898ac65e63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scaler re-saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\preprocessed_dataset_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(\"Attack_enc\", axis=1)\n",
    "\n",
    "# Create and fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Save the scaler again using your current env\n",
    "joblib.dump(scaler, r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\sgd_linear_svm_scaler.pkl\")\n",
    "\n",
    "print(\"âœ… Scaler re-saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4e72bc-e32d-4261-97a8-fea46648f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test data prepared successfully.\n",
      "ðŸ§ª X_test_scaled shape: (42534, 63)\n",
      "ðŸŽ¯ y_test_cat shape: (42534, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# === 1. Load the final preprocessed dataset ===\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\preprocessed_dataset_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === 2. Separate features and label ===\n",
    "X = df.drop('Attack_enc', axis=1)\n",
    "y = df['Attack_enc']\n",
    "\n",
    "# === 3. Train-test split (ensure same seed) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# === 4. Load the scaler used earlier ===\n",
    "scaler = joblib.load(r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\sgd_linear_svm_scaler.pkl\")\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === 5. One-hot encode labels ===\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "print(\"âœ… Test data prepared successfully.\")\n",
    "print(\"ðŸ§ª X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"ðŸŽ¯ y_test_cat shape:\", y_test_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a87399-02de-4f7c-b00a-e1dba8e017a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training data prepared.\n",
      "ðŸŸ¢ X_train_scaled shape: (99245, 63)\n",
      "ðŸŽ¯ y_train_cat shape: (99245, 18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "\n",
    "# === 1. Load preprocessed dataset ===\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\preprocessed_dataset_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === 2. Features and labels ===\n",
    "X = df.drop(\"Attack_enc\", axis=1)\n",
    "y = df[\"Attack_enc\"]\n",
    "\n",
    "# === 3. Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# === 4. Load the existing scaler used earlier ===\n",
    "scaler = joblib.load(r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\sgd_linear_svm_scaler.pkl\")\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# === 5. One-hot encode target ===\n",
    "y_train_cat = to_categorical(y_train)\n",
    "\n",
    "print(\"âœ… Training data prepared.\")\n",
    "print(\"ðŸŸ¢ X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"ðŸŽ¯ y_train_cat shape:\", y_train_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74481496-e468-4fc5-8649-379ce462eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "621/621 [==============================] - 6s 7ms/step - loss: 1.0051 - accuracy: 0.7304 - val_loss: 0.8665 - val_accuracy: 0.7506\n",
      "Epoch 2/30\n",
      "621/621 [==============================] - 5s 7ms/step - loss: 0.8642 - accuracy: 0.7540 - val_loss: 0.8412 - val_accuracy: 0.7658\n",
      "Epoch 3/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.8430 - accuracy: 0.7635 - val_loss: 0.8254 - val_accuracy: 0.7654\n",
      "Epoch 4/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.8228 - accuracy: 0.7676 - val_loss: 0.8163 - val_accuracy: 0.7670\n",
      "Epoch 5/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.8148 - accuracy: 0.7701 - val_loss: 0.8115 - val_accuracy: 0.7690\n",
      "Epoch 6/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.8104 - accuracy: 0.7712 - val_loss: 0.8102 - val_accuracy: 0.7697\n",
      "Epoch 7/30\n",
      "621/621 [==============================] - 3s 4ms/step - loss: 0.8051 - accuracy: 0.7716 - val_loss: 0.8093 - val_accuracy: 0.7714\n",
      "Epoch 8/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.8026 - accuracy: 0.7724 - val_loss: 0.8084 - val_accuracy: 0.7712\n",
      "Epoch 9/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.8003 - accuracy: 0.7735 - val_loss: 0.8070 - val_accuracy: 0.7700\n",
      "Epoch 10/30\n",
      "621/621 [==============================] - 5s 7ms/step - loss: 0.7993 - accuracy: 0.7733 - val_loss: 0.8067 - val_accuracy: 0.7717\n",
      "Epoch 11/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7990 - accuracy: 0.7744 - val_loss: 0.8070 - val_accuracy: 0.7712\n",
      "Epoch 12/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7967 - accuracy: 0.7747 - val_loss: 0.8055 - val_accuracy: 0.7711\n",
      "Epoch 13/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7950 - accuracy: 0.7745 - val_loss: 0.8031 - val_accuracy: 0.7737\n",
      "Epoch 14/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7945 - accuracy: 0.7746 - val_loss: 0.8039 - val_accuracy: 0.7735\n",
      "Epoch 15/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7931 - accuracy: 0.7748 - val_loss: 0.8014 - val_accuracy: 0.7737\n",
      "Epoch 16/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7921 - accuracy: 0.7757 - val_loss: 0.8032 - val_accuracy: 0.7726\n",
      "Epoch 17/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7925 - accuracy: 0.7758 - val_loss: 0.8032 - val_accuracy: 0.7728\n",
      "Epoch 18/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7915 - accuracy: 0.7755 - val_loss: 0.8007 - val_accuracy: 0.7724\n",
      "Epoch 19/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7913 - accuracy: 0.7754 - val_loss: 0.8054 - val_accuracy: 0.7703\n",
      "Epoch 20/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7903 - accuracy: 0.7759 - val_loss: 0.7993 - val_accuracy: 0.7732\n",
      "Epoch 21/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7886 - accuracy: 0.7763 - val_loss: 0.8002 - val_accuracy: 0.7736\n",
      "Epoch 22/30\n",
      "621/621 [==============================] - 5s 7ms/step - loss: 0.7916 - accuracy: 0.7759 - val_loss: 0.8032 - val_accuracy: 0.7708\n",
      "Epoch 23/30\n",
      "621/621 [==============================] - 4s 6ms/step - loss: 0.7892 - accuracy: 0.7765 - val_loss: 0.7997 - val_accuracy: 0.7739\n",
      "Epoch 24/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7881 - accuracy: 0.7762 - val_loss: 0.7995 - val_accuracy: 0.7737\n",
      "Epoch 25/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7876 - accuracy: 0.7760 - val_loss: 0.7981 - val_accuracy: 0.7730\n",
      "Epoch 26/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7876 - accuracy: 0.7768 - val_loss: 0.7959 - val_accuracy: 0.7728\n",
      "Epoch 27/30\n",
      "621/621 [==============================] - 4s 7ms/step - loss: 0.7878 - accuracy: 0.7760 - val_loss: 0.8005 - val_accuracy: 0.7719\n",
      "Epoch 28/30\n",
      "621/621 [==============================] - 3s 6ms/step - loss: 0.7867 - accuracy: 0.7765 - val_loss: 0.7957 - val_accuracy: 0.7739\n",
      "Epoch 29/30\n",
      "621/621 [==============================] - 3s 5ms/step - loss: 0.7869 - accuracy: 0.7767 - val_loss: 0.7959 - val_accuracy: 0.7725\n",
      "Epoch 30/30\n",
      "621/621 [==============================] - 5s 8ms/step - loss: 0.7861 - accuracy: 0.7771 - val_loss: 0.7938 - val_accuracy: 0.7741\n",
      "âœ… Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# === 1. Define a simple feedforward neural network ===\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# === 2. Train the model ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === 3. Save the trained model ===\n",
    "model.save(r\"C:\\Users\\Administrator\\Desktop\\Saif\\CIC_Modbus_Research\\Final_Clean_Merged\\multiclass\\my_dl_model.h5\")\n",
    "print(\"âœ… Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e0fcaf-fda9-4578-92e4-8e36b1430b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330/1330 [==============================] - 3s 2ms/step\n",
      "\n",
      "ðŸ“Š Evaluation Summary (Deep Learning):\n",
      "Metric                   Value\n",
      "----------------------------------------\n",
      "Accuracy                 0.777966\n",
      "Precision (Macro)        0.230145\n",
      "Recall (Macro)           0.186513\n",
      "F1-Score (Macro)         0.183030\n",
      "ROC-AUC (Macro)          0.901473\n",
      "ROC-AUC (Micro)          0.975205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# === Predict probabilities and classes ===\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# === Classification Metrics ===\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='macro', zero_division=0)\n",
    "\n",
    "# === ROC-AUC (macro & micro) ===\n",
    "roc_auc_macro = roc_auc_score(y_test_cat, y_pred_probs, average='macro', multi_class='ovr')\n",
    "roc_auc_micro = roc_auc_score(y_test_cat, y_pred_probs, average='micro', multi_class='ovr')\n",
    "\n",
    "# === Final Results Summary ===\n",
    "print(\"\\nðŸ“Š Evaluation Summary (Deep Learning):\")\n",
    "print(f\"{'Metric':<25}{'Value'}\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"{'Accuracy':<25}{accuracy:.6f}\")\n",
    "print(f\"{'Precision (Macro)':<25}{precision:.6f}\")\n",
    "print(f\"{'Recall (Macro)':<25}{recall:.6f}\")\n",
    "print(f\"{'F1-Score (Macro)':<25}{f1:.6f}\")\n",
    "print(f\"{'ROC-AUC (Macro)':<25}{roc_auc_macro:.6f}\")\n",
    "print(f\"{'ROC-AUC (Micro)':<25}{roc_auc_micro:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5116279b-b530-45c8-bd8d-d547409df736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330/1330 [==============================] - 4s 3ms/step\n",
      "âœ… Accuracy:               0.7780\n",
      "ðŸŽ¯ Precision (Macro):      0.2301  | (Micro): 0.7780\n",
      "ðŸ” Recall (Macro):         0.1865     | (Micro): 0.7780\n",
      "ðŸ“Š F1-Score (Macro):       0.1830         | (Micro): 0.7780\n",
      "ðŸ“ˆ ROC-AUC (Macro):        0.9015    | (Micro): 0.9752\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# === Step 1: Predict probabilities from your trained model ===\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "\n",
    "# === Step 2: Convert probabilities to class labels ===\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# === Step 3: Compute evaluation metrics ===\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "roc_auc_macro = roc_auc_score(y_test_cat, y_pred_proba, average='macro', multi_class='ovr')\n",
    "roc_auc_micro = roc_auc_score(y_test_cat, y_pred_proba, average='micro', multi_class='ovr')\n",
    "\n",
    "# === Step 4: Display results ===\n",
    "print(f\"âœ… Accuracy:               {accuracy:.4f}\")\n",
    "print(f\"ðŸŽ¯ Precision (Macro):      {precision_macro:.4f}  | (Micro): {precision_micro:.4f}\")\n",
    "print(f\"ðŸ” Recall (Macro):         {recall_macro:.4f}     | (Micro): {recall_micro:.4f}\")\n",
    "print(f\"ðŸ“Š F1-Score (Macro):       {f1_macro:.4f}         | (Micro): {f1_micro:.4f}\")\n",
    "print(f\"ðŸ“ˆ ROC-AUC (Macro):        {roc_auc_macro:.4f}    | (Micro): {roc_auc_micro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751b988-4ad9-4b54-b92e-2eba563a910b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf310env)",
   "language": "python",
   "name": "tf310env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
